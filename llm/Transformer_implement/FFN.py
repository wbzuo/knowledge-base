import torch
import torch.nn as nn
import torch.nn.functional as F  # ä¿®æ­£ï¼šåº”è¯¥æ˜¯ torch.nn.functional

class MLP(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, dropout):
        super().__init__()
        self.dim = dim
        self.hidden_dim = hidden_dim
        
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)
        self.dropout = nn.Dropout(p=dropout)
    
    def forward(self, x):
        x = F.relu(self.w1(x))  # ä¿®æ­£ï¼šrelu åº”è¯¥æ˜¯å°å†™
        x = self.w2(x)
        x = self.dropout(x)
        return x

def test_mlp():
    """æµ‹è¯• MLP æ¨¡å‹çš„å‡½æ•°"""
    print("=== æµ‹è¯• MLP æ¨¡å‹ ===")
    
    # æ¨¡å‹å‚æ•°
    dim = 512
    hidden_dim = 1024
    dropout = 0.1
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = MLP(dim=dim, hidden_dim=hidden_dim, dropout=dropout)
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    print("æ¨¡å‹ç»“æ„:")
    print(model)
    print(f"å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # æµ‹è¯•æ•°æ®
    batch_size = 4
    seq_len = 10
    test_input = torch.randn(batch_size, seq_len, dim)
    print(f"\nè¾“å…¥å½¢çŠ¶: {test_input.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(test_input)
    
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # éªŒè¯ç»´åº¦ä¸€è‡´æ€§
    assert output.shape == test_input.shape, f"è¾“å‡ºå½¢çŠ¶ {output.shape} ä¸è¾“å…¥å½¢çŠ¶ {test_input.shape} ä¸åŒ¹é…"
    print("âœ“ ç»´åº¦ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
    
    # æµ‹è¯• dropout æ•ˆæœ
    print(f"\n=== Dropout æµ‹è¯• ===")
    model.eval()  # è¯„ä¼°æ¨¡å¼ï¼ˆæ— dropoutï¼‰
    output_eval = model(test_input)
    
    model.train()  # è®­ç»ƒæ¨¡å¼ï¼ˆæœ‰dropoutï¼‰
    output_train = model(test_input)
    
    print(f"è¯„ä¼°æ¨¡å¼è¾“å‡ºæ–¹å·®: {output_eval.var().item():.4f}")
    print(f"è®­ç»ƒæ¨¡å¼è¾“å‡ºæ–¹å·®: {output_train.var().item():.4f}")
    print("âœ“ Dropout åŠŸèƒ½æ­£å¸¸")
    
    # å‚æ•°æ£€æŸ¥
    print(f"\n=== å‚æ•°æ£€æŸ¥ ===")
    for name, param in model.named_parameters():
        print(f"{name}: {param.shape}")
    
    return model, test_input, output

def test_mlp_different_configs():
    """æµ‹è¯•ä¸åŒé…ç½®çš„ MLP"""
    print("\n" + "="*50)
    print("æµ‹è¯•ä¸åŒé…ç½®çš„ MLP")
    print("="*50)
    
    test_cases = [
        {"dim": 256, "hidden_dim": 512, "dropout": 0.1},
        {"dim": 768, "hidden_dim": 3072, "dropout": 0.2},  # Transformer å¸¸ç”¨é…ç½®
        {"dim": 128, "hidden_dim": 256, "dropout": 0.0},
    ]
    
    for i, config in enumerate(test_cases):
        print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i+1} ---")
        print(f"é…ç½®: dim={config['dim']}, hidden_dim={config['hidden_dim']}, dropout={config['dropout']}")
        
        model = MLP(**config)
        test_input = torch.randn(2, 8, config['dim'])
        output = model(test_input)
        
        print(f"è¾“å…¥: {test_input.shape} -> è¾“å‡º: {output.shape}")
        print(f"å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")

if __name__ == "__main__":  # ä¿®æ­£ï¼šåº”è¯¥æ˜¯ __main__
    # è¿è¡Œæµ‹è¯•
    model, test_input, output = test_mlp()
    
    # æµ‹è¯•ä¸åŒé…ç½®
    test_mlp_different_configs()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼MLP æ¨¡å‹å·¥ä½œæ­£å¸¸ã€‚")